{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Ass5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7SdjfjSiswX"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "NLP Assignment 5\n",
        "Submitted by:\n",
        "  Aleezeh Usman\n",
        "  18I-0529\n",
        "  i180529@nu.edu.pk\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h30QSqIASAB5"
      },
      "source": [
        "# **Fake News Detection** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbNSGk7CcSmD"
      },
      "source": [
        "import required libraries and get data, then organize it in usable format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qngcFWyR9D1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af9ce16c-9b8d-4122-91d9-2a0ef618f390"
      },
      "source": [
        "!pip install spacy\n",
        "import csv\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (56.1.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5cK7VaZlA6f"
      },
      "source": [
        "#declaring important variables\n",
        "realcorpus = list()             #entire list of words in the real news corpus\n",
        "fakecorpus = list()             #entire list of words in the fake news corpus\n",
        "fakevocab = dict()              #this will store all unique words and the count of each word\n",
        "realvocab = dict()\n",
        "stopwords = list()\n",
        "realtestcorpus = list()\n",
        "faketestcorpus = list()\n",
        "punctuation = ['\\n','۔','،','!','؟','?','!','.','\"','’','‘','/','\\\\','%','٪','’’','‘‘']\n",
        "       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRkxMaAQITx4",
        "outputId": "46451104-785f-43c7-b389-49a4f3003521"
      },
      "source": [
        "with open(\"drive/My Drive/Classroom/Natural Language Processing (Spring 2021)/Notebooks/assignment_5/data/stopwords-ur.txt\", 'r') as f:\n",
        "  reader = csv.reader(f)\n",
        "  stopwords = list(reader)\n",
        "f.close()\n",
        "\n",
        "print(len(stopwords))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "517\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btidh-GHTCEY"
      },
      "source": [
        "#to read all the files in a folder get all the file names first\n",
        "realfileslist = os.listdir(\"drive/My Drive/Classroom/Natural Language Processing (Spring 2021)/Notebooks/assignment_5/data/Train/Real\")\n",
        "fakefileslist = os.listdir(\"drive/My Drive/Classroom/Natural Language Processing (Spring 2021)/Notebooks/assignment_5/data/Train/Fake\")\n",
        "realtestfiles = os.listdir(\"drive/My Drive/Classroom/Natural Language Processing (Spring 2021)/Notebooks/assignment_5/data/Test/Real\")\n",
        "faketestfiles = os.listdir(\"drive/My Drive/Classroom/Natural Language Processing (Spring 2021)/Notebooks/assignment_5/data/Test/Fake\")\n",
        "#print(len(realfileslist))\n",
        "#print(len(fakefileslist))\n",
        "#print(len(realtestfiles))\n",
        "#print(len(faketestfiles))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9aXXOKJU1eK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af160f87-5d41-42c6-9cae-ed861ea60eda"
      },
      "source": [
        "#now read contents of each file and store in a list \n",
        "for each in realfileslist:\n",
        "  path = 'drive/My Drive/Classroom/Natural Language Processing (Spring 2021)/Notebooks/assignment_5/data/Train/Real/' + each\n",
        "  with open(path, 'r') as f:\n",
        "    realfilecontent = f.read()\n",
        "  f.close()\n",
        "  realcorpus.append(realfilecontent) \n",
        "\n",
        "for each in fakefileslist:\n",
        "  path = 'drive/My Drive/Classroom/Natural Language Processing (Spring 2021)/Notebooks/assignment_5/data/Train/Fake/' + each\n",
        "  with open(path, 'r') as f:\n",
        "    fakefilecontent = f.read()\n",
        "  f.close()\n",
        "  fakecorpus.append(fakefilecontent) \n",
        "\n",
        "for each in realtestfiles:\n",
        "  path = 'drive/My Drive/Classroom/Natural Language Processing (Spring 2021)/Notebooks/assignment_5/data/Test/Real/' + each\n",
        "  with open(path, 'r') as f:\n",
        "    realtestcontent = f.read()\n",
        "  f.close()\n",
        "  realtestcorpus.append(realtestcontent)\n",
        "\n",
        "for each in faketestfiles:\n",
        "  path = 'drive/My Drive/Classroom/Natural Language Processing (Spring 2021)/Notebooks/assignment_5/data/Test/Real/' + each\n",
        "  with open(path, 'r') as f:\n",
        "    faketestcontent = f.read()\n",
        "  f.close()\n",
        "  faketestcorpus.append(faketestcontent)\n",
        "\n",
        "print(len(realtestcorpus))\n",
        "print(len(faketestcorpus))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "150\n",
            "112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktxpCk53fjp-"
      },
      "source": [
        "#this will generate count the occurences of each word to build a vocabulary that can be used later on\n",
        "def UnigramModel(training_set, total_words):\n",
        "  probabilities = dict()\n",
        "  i = 0\n",
        "\n",
        "  while(i < total_words):                               # in a single loop simply count occurences of each word\n",
        "    if str(training_set[i]) not in punctuation:\n",
        "      if str(training_set[i]) in probabilities:\n",
        "        probabilities[str(training_set[i])] += 1\n",
        "      else:\n",
        "        probabilities[str(training_set[i])] = 1\n",
        "    i += 1\n",
        "\n",
        "  return probabilities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBd2SxVWdBwE"
      },
      "source": [
        "import spacy as sp\n",
        "\n",
        "#convert entire real corpus to a realvocab\n",
        "tempstring = \"\"\n",
        "for each in realcorpus:\n",
        "  tempstring += each\n",
        "nlp = sp.blank('ur')\n",
        "tempvocab = nlp(tempstring)\n",
        "realvocab = dict()\n",
        "realvocab = UnigramModel(tempvocab, len(tempvocab))\n",
        "\n",
        "\n",
        "#convert entire fake corpus to a fake vocab\n",
        "tempstring = \"\"\n",
        "for each in fakecorpus:\n",
        "  tempstring += each\n",
        "nlp = sp.blank('ur')\n",
        "tempvocab = nlp(tempstring)\n",
        "fakevocab = dict()\n",
        "fakevocab = UnigramModel(tempvocab, len(tempvocab))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i29nV90KixJz"
      },
      "source": [
        "# **IMPLEMENTATION**\n",
        "Now we have contents of each document stored separately in realcorpus and fakecorpus, and contents of all the documents of the real files stored together as a single dictionary with their counts in realvocab and vice versa for fakevocab. Prior is a dictionary with information about the two classes but it is 0 right now, and condprob dict is declared but empty, , now we will begin the work. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqhX_g76ycxk"
      },
      "source": [
        "import spacy as sp\n",
        "\n",
        "#function for boolean NB implementation i.e to remove duplicate words from each document\n",
        "def ConvertToBooleanCorpus(corpus):\n",
        "  toreturncorpus = list()\n",
        "  nlp = sp.blank('ur')\n",
        "\n",
        "  for eachdoc in corpus:\n",
        "    tempstring = \"\"\n",
        "    tempstore = nlp(eachdoc)\n",
        "    for word in tempstore:\n",
        "      if str(word) not in tempstring:\n",
        "        tempstring += str(word) \n",
        "    toreturncorpus.append(tempstring)\n",
        "\n",
        "  return toreturncorpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT2C4FoDYadK"
      },
      "source": [
        "#Functions required in the Train Mulinomial NB function\n",
        "import spacy as sp\n",
        "\n",
        "def GetVocabulary(fakecorpus, realcorpus):\n",
        "  list_vocab = list()\n",
        "  nlp = sp.blank('ur')\n",
        "  \n",
        "  #store all words from real corpus in a list\n",
        "  for each in realcorpus:                  #get words in a sentence \n",
        "    tempvocab = nlp(each)\n",
        "    for word in tempvocab:                  #now append each word individually in the list storing words\n",
        "      if str(word) not in punctuation:\n",
        "       list_vocab.append(str(word))\n",
        "  #store all words from fake corpus in a list\n",
        "  for each in fakecorpus:\n",
        "    tempvocab = nlp(each)\n",
        "    for word in tempvocab:\n",
        "      if str(word) not in punctuation:\n",
        "        list_vocab.append(str(word))\n",
        "\n",
        "  return list_vocab\n",
        "\n",
        "def CountWordsInAllTextsofClass(vocab):\n",
        "  count = 0\n",
        "  for each in vocab:\n",
        "    count += vocab[each]\n",
        "  return count\n",
        "\n",
        "def CountTokensOfWords(doc_c, w):\n",
        "  count = 0\n",
        "  if w in doc_c:\n",
        "    count = doc_c[w]\n",
        "  return count\n",
        "\n",
        "def ExtractWordsfromText(textdoc):\n",
        "  toreturn = list()\n",
        "  nlp = sp.blank('ur')\n",
        "  tempvocab = nlp(textdoc)\n",
        "  for word in tempvocab:    \n",
        "    if str(word) not in punctuation:\n",
        "      toreturn.append(str(word))\n",
        "\n",
        "  return toreturn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO19FgcMbuGl"
      },
      "source": [
        "def InternalLoopforTrainMultinomailNB(V, vocab):\n",
        "  condprob = dict()\n",
        "  Nw = CountWordsInAllTextsofClass(vocab) #number of total words in all the text files in the corpus of that class\n",
        "  for eachword in vocab:\n",
        "    Ni = CountTokensOfWords(vocab, eachword)\n",
        "    cp = (Ni+1)/(Nw + len(V))\n",
        "    condprob[eachword] = cp               #each word will store the corresponding probability of having that word in the given class\n",
        "  \n",
        "  toReturnList =  condprob\n",
        "  return toReturnList"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxxpmwPhuON0"
      },
      "source": [
        "def TrainMultinomialNB(realvocab, fakevocab, realcorpus, fakecorpus, stopwords, removestopwords):\n",
        "  N = len(realcorpus) + len(fakecorpus)               #total number of text documents in both classes\n",
        "  V = GetVocabulary(fakecorpus, realcorpus)           #Complete Vocabulary with duplicates and stopwords too\n",
        "  if removestopwords == 1:                            #remove stopwords or not\n",
        "    for each in stopwords:\n",
        "      if each[0] in V:\n",
        "        try:\n",
        "          while True:\n",
        "            V.remove(each[0])\n",
        "        except ValueError:\n",
        "          pass\n",
        "  print(len(V))\n",
        "\n",
        "  #Calculate everything for real news\n",
        "  Nc = len(realcorpus)                                #number of text documents of the real news corpus\n",
        "  prior['R'] = Nc/N                                   #calculate prior \n",
        "  GetInfo = InternalLoopforTrainMultinomailNB(V,realvocab)  \n",
        "  condprob['R'] = GetInfo.copy()                      #store entire dict containing cond prob of each word being in real class\n",
        "\n",
        "  #Calculate everything for fake news\n",
        "  Nc = len(fakecorpus)                                #number of text documents of the real news corpus\n",
        "  prior['F'] = Nc/N\n",
        "  GetInfo = InternalLoopforTrainMultinomailNB(V,fakevocab)\n",
        "  condprob['F'] = GetInfo.copy()                      #store entire dict containing cond prob of each word being in fake class\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3Xd5XEqfS8i"
      },
      "source": [
        "from math import log\n",
        "\n",
        "def NaiveBayesModel(prior, condprob, testcorpus, stopwords, removestopwords):\n",
        "  W = ExtractWordsfromText(testcorpus)\n",
        "  if removestopwords == 1:\n",
        "    for each in stopwords:                        #remove stopwords if required\n",
        "      if each[0] in W:\n",
        "        try:\n",
        "          while True:\n",
        "            W.remove(each[0])\n",
        "        except ValueError:\n",
        "          pass\n",
        "  score = dict()\n",
        "\n",
        "  score['R'] = log(prior['R'])                  #calculate score of the doc as part of real(R) class\n",
        "  for w in W:\n",
        "    if w in condprob['R']:\n",
        "      score['R'] += log(condprob['R'][w])\n",
        "  \n",
        "  score['F'] = log(prior['F'])                  #calculate score of the doc as part of fake(F) class\n",
        "  for w in W:\n",
        "    if w in condprob['F']:\n",
        "      score['F'] += log(condprob['F'][w])\n",
        "  \n",
        "  if score['F'] > score['R']:                   #return the correct label based on max score\n",
        "    return 'F'\n",
        "  else:\n",
        "    return 'R'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBAlpf_gto0P"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def EvaluateCode(realtestcorpus,faketestcorpus, prior, condprob, stopwords, removestopwords):\n",
        "  predicted_labels = list()                             #store labels that we should be getting\n",
        "  actual_labels = list()                                #store labels that we actually got from our algorithm\n",
        "\n",
        "  for doc in realtestcorpus:                            #get label for each document in our data set separately\n",
        "    decision = NaiveBayesModel(prior, condprob, doc, stopwords, removestopwords)\n",
        "    predicted_labels.append(decision)\n",
        "    actual_labels.append('R')                           #this loop is for real news documents so predicted label is R\n",
        "\n",
        "  for doc in faketestcorpus:\n",
        "    decision = NaiveBayesModel(prior, condprob, doc, stopwords, removestopwords)\n",
        "    predicted_labels.append(decision)\n",
        "    actual_labels.append('F')                           #this loop is for fake news documents so predicted label is F\n",
        "    \n",
        "  #evaluate and store the scores of evaluation measures\n",
        "  accuracy = accuracy_score(actual_labels, predicted_labels)\n",
        "  precision = precision_score(actual_labels, predicted_labels, average = 'macro')\n",
        "  recall = recall_score(actual_labels, predicted_labels, average='macro')\n",
        "  f1score = f1_score(actual_labels, predicted_labels, average='macro')\n",
        "  #display evaluation results\n",
        "  print(\"Accuracy: \", end = \"\")\n",
        "  print(accuracy)\n",
        "  print(\"Precision: \", end = \"\")\n",
        "  print(precision)\n",
        "  print(\"Recall: \", end = \"\")\n",
        "  print(recall)\n",
        "  print(\"F1 Measure: \", end = \"\")\n",
        "  print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTRq49lA7_FE"
      },
      "source": [
        "# **FINAL IMPLEMENTATION - WITHOUT STOP WORDS REMOVAL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwC4RkOyvWvk",
        "outputId": "52af2d25-3611-41e6-c661-96373791a45a"
      },
      "source": [
        "prior = {\"R\": 0, \"F\":0}         #this is our basic distinct classes and will store the prior value\n",
        "condprob = dict()               # we will calculate the cond probability of each word later\n",
        "removestopwords = 0\n",
        "TrainMultinomialNB(realvocab,fakevocab,realcorpus,fakecorpus,stopwords,removestopwords)\n",
        "print(prior)\n",
        "EvaluateCode(realtestcorpus,faketestcorpus,prior,condprob,stopwords,removestopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "215382\n",
            "{'R': 0.54858934169279, 'F': 0.45141065830721006}\n",
            "Accuracy: 0.48091603053435117\n",
            "Precision: 0.49124999999999996\n",
            "Recall: 0.49124999999999996\n",
            "F1 Measure: 0.48091603053435117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qZkKpAwBAqr"
      },
      "source": [
        "# **FINAL IMPLEMENTATION - WITHOUT STOPWORDS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwH0ENuEBGwJ",
        "outputId": "d15e7bb8-74c0-4fcc-c0cc-25e955b64e7a"
      },
      "source": [
        "#remove stopwords from the real and fake news vocabularies\n",
        "for each in stopwords:\n",
        "  if each[0] in realvocab:\n",
        "    realvocab.pop(each[0])\n",
        "  if each[0] in fakevocab:\n",
        "    fakevocab.pop(each[0])\n",
        "#print(len(realvocab))\n",
        "#print(len(fakevocab))\n",
        "\n",
        "prior = {\"R\": 0, \"F\":0}         #this is our basic distinct classes and will store the prior value\n",
        "condprob = dict()               # we will calculate the cond probability of each word later\n",
        "removestopwords = 1\n",
        "TrainMultinomialNB(realvocab,fakevocab,realcorpus,fakecorpus,stopwords,removestopwords)\n",
        "print(prior)\n",
        "EvaluateCode(realtestcorpus,faketestcorpus,prior,condprob,stopwords,removestopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "167168\n",
            "{'R': 0.54858934169279, 'F': 0.45141065830721006}\n",
            "Accuracy: 0.46564885496183206\n",
            "Precision: 0.49043606206527557\n",
            "Recall: 0.4914880952380952\n",
            "F1 Measure: 0.46564885496183206\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCC1QV_qTe2G"
      },
      "source": [
        "# **FINAL IMPLEMENTATION - BOOLEAN NB**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdvG0XCETc4u"
      },
      "source": [
        "booleanrealcorpus = list()\n",
        "booleanfakecorpus = list()\n",
        "\n",
        "booleanrealcorpus = ConvertToBooleanCorpus(realcorpus)\n",
        "booleanfakecorpus = ConvertToBooleanCorpus(fakecorpus)\n",
        "\n",
        "import spacy as sp\n",
        "#convert entire boolean real corpus to a boolean realvocab\n",
        "tempstring = \"\"\n",
        "for each in booleanrealcorpus:\n",
        "  tempstring += each\n",
        "nlp = sp.blank('ur')\n",
        "tempvocab = nlp(tempstring)\n",
        "booleanrealvocab = dict()\n",
        "booleanrealvocab = UnigramModel(tempvocab, len(tempvocab))\n",
        "#convert entire boolean fake corpus to a boolean fake vocab\n",
        "tempstring = \"\"\n",
        "for each in booleanfakecorpus:\n",
        "  tempstring += each\n",
        "nlp = sp.blank('ur')\n",
        "tempvocab = nlp(tempstring)\n",
        "booleanfakevocab = dict()\n",
        "booleanfakevocab = UnigramModel(tempvocab, len(tempvocab))\n",
        "\n",
        "#remove stopwords from fake and real boolean vocab\n",
        "for each in stopwords:\n",
        "  if each[0] in booleanrealvocab:\n",
        "    booleanrealvocab.pop(each[0])\n",
        "  if each[0] in booleanfakevocab:\n",
        "    booleanfakevocab.pop(each[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHwQ9xJl96eO"
      },
      "source": [
        "#set up test corpuses  for boolean as well\n",
        "booleanrealtestcorpus = list()\n",
        "booleanfaketestcorpus = list()\n",
        "booleanfaketestcorpus = ConvertToBooleanCorpus(faketestcorpus)\n",
        "booleanrealtestcorpus = ConvertToBooleanCorpus(realtestcorpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzoJReaq9ojD",
        "outputId": "9a94e852-1114-4d29-96ff-3425ee9ac00d"
      },
      "source": [
        "prior = {\"R\": 0, \"F\":0}         #this is our basic distinct classes and will store the prior value\n",
        "condprob = dict()               # we will calculate the cond probability of each word later\n",
        "removestopwords = 1\n",
        "TrainMultinomialNB(booleanrealvocab,booleanfakevocab,booleanrealcorpus,booleanfakecorpus,stopwords,removestopwords)\n",
        "print(prior)\n",
        "EvaluateCode(booleanrealtestcorpus,booleanfaketestcorpus,prior,condprob,stopwords,removestopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1884\n",
            "{'R': 0.54858934169279, 'F': 0.45141065830721006}\n",
            "Accuracy: 0.5534351145038168\n",
            "Precision: 0.4923042770460263\n",
            "Recall: 0.4969047619047619\n",
            "F1 Measure: 0.5534351145038168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-AJNirsCvTE"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "From the above results we can see that by removing duplicates in a file i.e Boolean Naive Bayes gives us better results as compared to simple Naive Bayes and also better than simply removing stop words. \n",
        "According to my results only removing stop words has very less change in the results. \n",
        "```\n",
        "\n"
      ]
    }
  ]
}